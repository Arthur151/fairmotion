{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these before executing\n",
    "# module load anaconda3/5.0.1\n",
    "# source activate fair_env_latest_py3_theano\n",
    "# module load cuda/9.0\n",
    "# module load NCCL/2.3.7-1-cuda.9.0\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from mocap_processing.motion.pfnn import Animation, BVH\n",
    "from mocap_processing.motion.pfnn.Quaternions import Quaternions\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANGLES = [45, 90, 135, 180]\n",
    "FREQUENCIES = [30, 60, 90, 120]\n",
    "TYPES = [\"sinusoidal\", \"sinusoidal_x\"] #'pendulum'\n",
    "NUM_JOINTS = 4\n",
    "NUM_COORDINATES = 3\n",
    "NUM_PREDICTIONS = 2 * NUM_JOINTS * NUM_COORDINATES\n",
    "BASE_PATH = \"/checkpoint/dgopinath/natural_gesture_generation/data/\"\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 128\n",
    "SUBSAMPLED_FRAMERATE = 30\n",
    "ACTION_TIME = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positions_wrt_base(anim):\n",
    "\n",
    "    # global transforms for each frame F and joint J\n",
    "    globals = Animation.transforms_global(anim)  # (F, J, 4, 4) ndarray\n",
    "    transforms_wrt_root = Animation.transforms_blank(anim)\n",
    "\n",
    "    for i in range(1, anim.shape[1]):  # modifies all joints except root/base\n",
    "        transforms_wrt_root[:, i] = Animation.transforms_multiply(Animation.transforms_inv(globals[:, 0]), globals[:, i])\n",
    "    transforms_wrt_root[:, 0] = globals[:, 0]  # root/base joint retains global position\n",
    "\n",
    "    positions_wrt_root = transforms_wrt_root[:, :, 0:3, 3]\n",
    "    rotations_wrt_root = Quaternions.from_transforms(transforms_wrt_root).angle_axis()\n",
    "\n",
    "    return rotations_wrt_root, positions_wrt_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGED FORMAT TO (ALL_ROTATIONS, ALL_POSITIONS)\n",
    "\n",
    "def extract_sequences(joint_rotations, joint_positions, frametime,\n",
    "                      stride=0.25, downsampled_hertz=30, window_length=5.0):\n",
    "\n",
    "    # initialize data structs, using CNN-like computation of size of input\n",
    "    J_p, J_r = joint_positions.shape[1], joint_rotations.shape[1],  # num of joints\n",
    "    animation_time = len(joint_rotations) * frametime\n",
    "    n = int(np.trunc((animation_time - window_length) / stride)) + 1  # sample size\n",
    "    num_joint_coordinates = joint_rotations.shape[2] + joint_positions.shape[2]\n",
    "    num_posture_dimensions = (J_p * joint_positions.shape[2]) + (J_r * joint_rotations.shape[2])  #J * num_joint_coordinates\n",
    "    T = int(np.trunc(downsampled_hertz * window_length))\n",
    "\n",
    "    datapoint_size = num_posture_dimensions * T\n",
    "    datapoints = np.zeros((n, datapoint_size), dtype=float)\n",
    "    datapoints_temporal = np.zeros((n, T, num_posture_dimensions), dtype=float)\n",
    "\n",
    "\n",
    "    current_frame_iterator = 0\n",
    "    input_hertz = int(np.trunc(1.0 / frametime))\n",
    "    step_size = int(np.trunc(input_hertz / downsampled_hertz))\n",
    "    sample_joint_positions = np.zeros((n, T, J_p, joint_positions.shape[2]))\n",
    "    sample_joint_rotations = np.zeros((n, T, J_r, joint_rotations.shape[2]))\n",
    "\n",
    "    for i in range(n):\n",
    "        # retrieve downsampled set of frames, for extracting character body posture over duration of sequence\n",
    "        posture_sequence_concatenated = []\n",
    "        posture_sequence_by_time = np.zeros((T, num_posture_dimensions))\n",
    "        for t in range(current_frame_iterator, (T * step_size + current_frame_iterator), step_size):\n",
    "\n",
    "            # get posture information for given (target) frame\n",
    "            posture = []\n",
    "            t_downsampled = int((t - current_frame_iterator)/float(step_size))\n",
    "\n",
    "            if J_p == J_r:\n",
    "                J = J_p\n",
    "                for j in range(J):  # all joints\n",
    "                    # rotation data\n",
    "                    sample_joint_rotations[i][t_downsampled][j] = joint_rotations[t][j]\n",
    "                    posture.extend(joint_rotations[t][j])\n",
    "                for j in range(J):  # all joints\n",
    "                    # position data\n",
    "                    sample_joint_positions[i][t_downsampled][j] = joint_positions[t][j]\n",
    "                    posture.extend(joint_positions[t][j])\n",
    "            else:\n",
    "                joints_pos, joints_rot = set(list(range(J_p))), set(list(range(J_r)))\n",
    "                J = joints_pos | joints_rot  # union of sets\n",
    "                for j in J:\n",
    "                    if j in joints_pos:  # joints considered for position data\n",
    "                        sample_joint_positions[i][t_downsampled][j] = joint_positions[t][j]\n",
    "                        posture.extend(joint_positions[t][j])\n",
    "                    if j in joints_rot:  # joints considered for rotation data\n",
    "                        sample_joint_rotations[i][t_downsampled][j] = joint_rotations[t][j]\n",
    "                        posture.extend(joint_rotations[t][j])\n",
    "\n",
    "            posture_sequence_concatenated.extend(posture)\n",
    "            posture_sequence_by_time[t_downsampled] = posture\n",
    "\n",
    "\n",
    "        # update datapoints struct with current iteration input\n",
    "        datapoints[i] = posture_sequence_concatenated\n",
    "        datapoints_temporal[i] = posture_sequence_by_time\n",
    "        current_frame_iterator += int(np.trunc(stride * input_hertz))\n",
    "\n",
    "    return datapoints, datapoints_temporal, sample_joint_rotations, sample_joint_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/dgopinath/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = defaultdict(list)\n",
    "for angle, freq, motion_type in product(ANGLES, FREQUENCIES, TYPES):\n",
    "    filename = os.path.join(BASE_PATH, motion_type, \"source\", f\"{angle}_{freq}.bvh\")\n",
    "    anim, joint_names, frametime = BVH.load(filename)\n",
    "    joint_rotations_root, joint_positions_root = positions_wrt_base(anim)\n",
    "    _, input_data, _, _ = extract_sequences(joint_rotations_root[1], joint_positions_root, frametime)\n",
    "    # input_data is of shape [num_sequences, seq_len, NUM_PREDICTIONS]\n",
    "    train, test = model_selection.train_test_split(input_data, train_size=0.8)\n",
    "    dataset[\"train\"].extend(train)\n",
    "    dataset[\"test\"].extend(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "        # self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, batch):\n",
    "        lstm_out, (lstm_hidden, lstm_cell) = self.lstm(batch) # batch.view(-1, , NUM_PREDICTIONS))\n",
    "        return lstm_hidden, lstm_cell\n",
    "\n",
    "    \n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        output = output.squeeze(0)\n",
    "        output = self.out(output)\n",
    "        return output, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = 'cuda'\n",
    "                \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        teacher_forcing_ratio = 0.5\n",
    "        max_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "        outputs = torch.zeros(max_len, batch_size, NUM_PREDICTIONS).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0,:]\n",
    "        for t in range(1, max_len):\n",
    "            input = input.unsqueeze(0)\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[t] if teacher_force else output\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=1):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        _, (lstm_hidden, lstm_cell) = self.lstm(batch)\n",
    "        return lstm_hidden, lstm_cell\n",
    "\n",
    "\n",
    "class DecoderHelper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(DecoderHelper, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden=None, cell=None):\n",
    "        if (hidden is None) and (cell is None):\n",
    "            output, (hidden, cell) = self.lstm(input)\n",
    "        else:\n",
    "            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        output = output.squeeze(0)\n",
    "        output = self.out(output)\n",
    "        # output B, M*J\n",
    "        return output, hidden, cell\n",
    "\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, device='cuda'):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.decoder = DecoderHelper(input_dim, output_dim, hidden_dim)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, trg, hidden=None, cell=None):\n",
    "        # trg B, T, M*J\n",
    "        teacher_forcing_ratio = 0.5\n",
    "        trg = trg.transpose(0, 1)\n",
    "        # T, B, M*J\n",
    "        max_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "\n",
    "        input = trg[0, :]\n",
    "        outputs = torch.zeros(max_len, batch_size, self.decoder.input_dim).to(self.device)\n",
    "        for t in range(1, max_len):\n",
    "            input = input.unsqueeze(0)\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[t] if teacher_force else output\n",
    "            \n",
    "        # outputs T, B, M*J\n",
    "        outputs = outputs.transpose(0, 1)\n",
    "        # outputs B, T, M*J\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        hidden, cell = self.encoder(src)\n",
    "        outputs = self.decoder(trg, hidden, cell)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): LSTMEncoder(\n",
       "    (lstm): LSTM(24, 128, batch_first=True)\n",
       "  )\n",
       "  (decoder): LSTMDecoder(\n",
       "    (decoder): DecoderHelper(\n",
       "      (lstm): LSTM(24, 128)\n",
       "      (out): Linear(in_features=128, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = LSTMEncoder(NUM_PREDICTIONS, HIDDEN_DIM)\n",
    "dec = LSTMDecoder(NUM_PREDICTIONS, NUM_PREDICTIONS, HIDDEN_DIM)\n",
    "model = Seq2Seq(enc, dec).to('cuda')\n",
    "model.zero_grad()\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): LSTMEncoder(\n",
       "    (lstm): LSTM(24, 128, batch_first=True)\n",
       "  )\n",
       "  (decoder): LSTMDecoder(\n",
       "    (decoder): DecoderHelper(\n",
       "      (lstm): LSTM(24, 128)\n",
       "      (out): Linear(in_features=128, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    model.eval()\n",
    "    sequences = dataset[\"test\"]\n",
    "    with torch.no_grad():\n",
    "        batched_test_data_np = np.array(sequences)\n",
    "        batched_test_data_t = torch.from_numpy(batched_test_data_np).to(device='cuda')\n",
    "        batched_test_target_data_t = batched_test_data_t.transpose(0, 1)\n",
    "        outputs = model(batched_data_t, batched_target_data_t)\n",
    "        outputs = outputs.to(dtype=torch.double)\n",
    "        loss = criterion(outputs, batched_target_data_t)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, sequences):\n",
    "    model.eval()\n",
    "    batched_test_data_np = np.array(sequences)\n",
    "    batched_test_data_t = torch.from_numpy(batched_test_data_np).to(device='cuda')\n",
    "    batched_test_target_data_t = batched_test_data_t.transpose(0, 1)\n",
    "    outputs = model(batched_test_data_t, batched_test_target_data_t)\n",
    "    return outputs.transpose(0, 1).cpu().data.numpy(), batched_test_data_t.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n",
      "torch.Size([1, 64, 24])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input and target shapes do not match: input [150 x 64 x 24], target [64 x 150 x 24] at /private/home/cluster_monitor/pytorch-src/aten/src/THCUNN/generic/MSECriterion.cu:15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-d4c30be6b9e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_data_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_target_data_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_target_data_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# retain_graph=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fair_env_latest_py3_theano/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fair_env_latest_py3_theano/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fair_env_latest_py3_theano/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     \"\"\"\n\u001b[1;32m   1581\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[0;32m-> 1582\u001b[0;31m                            input, target, size_average, reduce)\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fair_env_latest_py3_theano/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[0;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input and target shapes do not match: input [150 x 64 x 24], target [64 x 150 x 24] at /private/home/cluster_monitor/pytorch-src/aten/src/THCUNN/generic/MSECriterion.cu:15"
     ]
    }
   ],
   "source": [
    "with open(\"/private/home/dgopinath/data/seq2seq/seq2seq.log\", \"w\") as f:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3)\n",
    "    for epoch in range(1):\n",
    "        f.write(f\"Epoch {epoch}\\n\")\n",
    "        f.write(f\"Dataset size {len(dataset['train'])}\\n\")\n",
    "        sequences = dataset[\"train\"]\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for i in range(int(len(sequences)/BATCH_SIZE)):\n",
    "            batched_data_np = np.array(sequences[i*BATCH_SIZE:(i+1)*BATCH_SIZE])\n",
    "            batched_data_t = torch.from_numpy(batched_data_np).to(device='cuda')\n",
    "            batched_target_data_t = batched_data_t #.transpose(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batched_data_t, batched_target_data_t)\n",
    "            outputs = outputs.to(dtype=torch.double)\n",
    "            loss = criterion(outputs, batched_target_data_t)\n",
    "            loss.backward() # retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            break\n",
    "        epoch_loss = epoch_loss/(len(sequences)/BATCH_SIZE)\n",
    "        f.write(f\"Training loss {epoch_loss}\\n\")\n",
    "        val_loss = eval(model)\n",
    "        f.write(f\"Validation loss {val_loss}\\n\")\n",
    "        scheduler.step(val_loss)\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(model.state_dict(), f\"/private/home/dgopinath/data/seq2seq/models/{epoch}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "training_loss, validation_loss = [], []\n",
    "with open(\"/private/home/dgopinath/data/seq2seq/seq2seq.log\") as f:\n",
    "    for line in f:\n",
    "        if \"Epoch\" in line or \"Dataset\" in line:\n",
    "            continue\n",
    "        if \"Training loss\" in line:\n",
    "            training_loss.append(float(line.split(\" \")[2]))\n",
    "        if \"Validation loss\" in line:\n",
    "            validation_loss.append(float(line.split(\" \")[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(training_loss)), training_loss)\n",
    "plt.plot(range(len(training_loss)), validation_loss)\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.savefig(\"/private/home/dgopinath/data/seq2seq_1layer_batch64.svg\", format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/private/home/dgopinath/data/seq2seq_losses.csv\", \"w\") as f:\n",
    "    f.write(f\"Epoch,Training_Loss,Validation_Loss\\n\")\n",
    "    for num, (t_loss, v_loss) in enumerate(zip(training_loss, validation_loss)):\n",
    "        f.write(f\"{num},{t_loss},{v_loss}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_data(tensor_struct):\n",
    "    if isinstance(tensor_struct, np.ndarray):\n",
    "        return tensor_struct\n",
    "\n",
    "    elif isinstance(tensor_struct, list) or isinstance(tensor_struct, tuple):\n",
    "        return np.array(tensor_struct)\n",
    "\n",
    "    elif torch.is_tensor(tensor_struct):\n",
    "        if tensor_struct.device.type == 'cuda':\n",
    "            data_np = tensor_struct.data.cpu().numpy()\n",
    "        else:\n",
    "            data_np = tensor_struct.data.numpy()\n",
    "        return data_np\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positions_global_from_base(rotations_joint_wrt_base, positions_joint_wrt_base):\n",
    "    \"\"\"\n",
    "    At each time t, for each j in J:  P_j^(global) = R_base * P_j^(base)  + P_base\n",
    "    :param anim:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    positions_base = positions_joint_wrt_base[:, 0]\n",
    "    rotations_base_transforms = rotations_joint_wrt_base.transforms()[:, 0]\n",
    "    T, J = positions_joint_wrt_base.shape[0], positions_joint_wrt_base.shape[1]\n",
    "    positions_global = np.zeros((T, J, positions_joint_wrt_base.shape[2]))\n",
    "\n",
    "    for t in range(T):\n",
    "        positions_global[t, 0] = positions_base[t]  # simply copy base joint position\n",
    "        for j in range(1, J):\n",
    "            positions_global[t,j] = np.matmul(rotations_base_transforms[t], positions_joint_wrt_base[t,j]) + \\\n",
    "                positions_base[t]\n",
    "            pass\n",
    "    return positions_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_motion_sequence(T, current_hertz, desired_hertz, joint_rotations, joint_positions):\n",
    "    upsample_rate = int(desired_hertz / current_hertz)\n",
    "    T_upsampled = T * upsample_rate\n",
    "    J_p, J_r = joint_positions.shape[1], joint_rotations.shape[1]\n",
    "    num_position_coordinates, num_rotation_coordinates = joint_positions.shape[2], joint_rotations.shape[2]\n",
    "\n",
    "\n",
    "    joint_rotations_upsampled = np.zeros(shape=(T_upsampled, J_r, num_rotation_coordinates))\n",
    "    joint_positions_upsampled = np.zeros(shape=(T_upsampled, J_p, num_position_coordinates))\n",
    "\n",
    "    for t in range(T):\n",
    "\n",
    "        t_upsampled = t * upsample_rate\n",
    "        joint_rotations_upsampled[t_upsampled] = joint_rotations[t]\n",
    "        joint_positions_upsampled[t_upsampled] = joint_positions[t]\n",
    "\n",
    "        if (t + 1) < T:\n",
    "            interpolated_rot_increment = (joint_rotations[t + 1] - joint_rotations[t]) / upsample_rate\n",
    "            interpolated_pos_increment = (joint_positions[t + 1] - joint_positions[t]) / upsample_rate\n",
    "            for i in range(1, upsample_rate):\n",
    "                joint_rotations_upsampled[t_upsampled + i] = joint_rotations[t] + i * interpolated_rot_increment\n",
    "                joint_positions_upsampled[t_upsampled + i] = joint_positions[t] + i * interpolated_pos_increment\n",
    "\n",
    "            pass\n",
    "\n",
    "    return joint_rotations_upsampled, joint_positions_upsampled\n",
    "\n",
    "    \n",
    "def decompose_axis_angles(axis_angles):\n",
    "    \"\"\"\n",
    "    Input dimensions: T x J x num-coordinates\n",
    "    Output dimensions:\n",
    "        angles: T x J\n",
    "        axes: T x J x num-coords\n",
    "    \"\"\"\n",
    "\n",
    "    angles = np.linalg.norm(get_tensor_data(axis_angles), axis=2)\n",
    "    axes = np.zeros_like(axis_angles)\n",
    "\n",
    "    for t in range(axis_angles.shape[0]):  # traverse through time/datapoints\n",
    "        for j in range(axis_angles.shape[1]):  # traverse through joints\n",
    "            if angles[t, j] < 1e-5:\n",
    "                axes[t, j, :] = [1, 0, 0]\n",
    "            else:\n",
    "                axes[t, j, :] = axis_angles[t, j, :] / angles[t, j]\n",
    "    return angles, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character():\n",
    "    \"\"\"Defines character profile and metadata\"\"\"\n",
    "\n",
    "    def __init__(self, id, orients=None, offsets=None, parents=None):\n",
    "\n",
    "        self.id = id\n",
    "\n",
    "        self.orients = orients\n",
    "        self.offsets = offsets\n",
    "        self.parents = parents\n",
    "\n",
    "\n",
    "        self.J_p = 0\n",
    "        self.J_r = 0\n",
    "        self.num_rotation_coordinates = 3\n",
    "        self.num_position_coordinates = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### Setters ####\n",
    "    def update_metadata(self, anim):\n",
    "        self.set_orients(anim.orients)\n",
    "        self.set_offsets(anim.offsets)\n",
    "        self.set_parents(anim.parents)\n",
    "\n",
    "\n",
    "    def set_orients(self, orients):\n",
    "        self.orients = orients\n",
    "\n",
    "\n",
    "    def set_offsets(self, offsets):\n",
    "        self.offsets = offsets\n",
    "\n",
    "\n",
    "    def set_parents(self, parents):\n",
    "        self.parents = parents\n",
    "\n",
    "\n",
    "\n",
    "    def set_joint_information(self, positions, rotations):\n",
    "\n",
    "        self.J_p = positions.shape[2]\n",
    "        self.num_position_coordinates = positions.shape[3]\n",
    "\n",
    "        self.J_r = rotations.shape[2]\n",
    "        self.num_rotation_coordinates = rotations.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LSTMEncoder(NUM_PREDICTIONS, HIDDEN_DIM)\n",
    "dec = LSTMDecoder(NUM_PREDICTIONS, HIDDEN_DIM, NUM_PREDICTIONS)\n",
    "loaded_model = Seq2Seq(enc, dec).to('cuda')\n",
    "loaded_model.load_state_dict(torch.load(\"/private/home/dgopinath/data/seq2seq/models/100.model\"))\n",
    "loaded_model.eval()\n",
    "loaded_model.double()\n",
    "\n",
    "outputs, inputs = generate(loaded_model, dataset[\"test\"])\n",
    "# source_character = Character('source')\n",
    "# T = len(joint_rotations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(outputs):\n",
    "    T = outputs.shape[1]\n",
    "    upsampled_sequences = []\n",
    "    for seq_idx in range(outputs.shape[0]):\n",
    "        rotations = outputs[seq_idx, :, 0:int(NUM_PREDICTIONS/2)].reshape(T, -1, 3)\n",
    "        positions = outputs[seq_idx, :, int(NUM_PREDICTIONS/2): NUM_PREDICTIONS].reshape(T, -1, 3)\n",
    "        rotations_wrt_base, positions_wrt_base = upsample_motion_sequence(T, 30, 120, rotations, positions)\n",
    "        joint_rotations_angles, joint_rotations_axis = decompose_axis_angles(rotations_wrt_base)\n",
    "        joint_rotations_quats = Quaternions.from_angle_axis(joint_rotations_angles, joint_rotations_axis)\n",
    "        upsampled_sequences.append((joint_rotations_quats, positions_wrt_base))\n",
    "    return upsampled_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalize(outputs, upsampled_sequences):\n",
    "    global_sequences = []\n",
    "    for seq_idx in range(outputs.shape[0]):\n",
    "        global_sequences.append(positions_global_from_base(*upsampled_sequences[seq_idx]))\n",
    "    return global_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bvh(globalized, upsampled, idx, filepath, input_file=BASE_PATH + \"/sinusoidal/source/90_90.bvh\"):\n",
    "    source_character = Character('source')\n",
    "    anim, _, _ = BVH.load(input_file)\n",
    "    source_character.update_metadata(anim)\n",
    "    input_anim = Animation.Animation(upsampled[idx][0], globalized[idx], source_character.orients, source_character.offsets, source_character.parents)\n",
    "    BVH.save(filepath, input_anim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled = upsample(inputs)\n",
    "globalized = globalize(inputs, upsampled)\n",
    "save_bvh(globalized, upsampled, 1, \"/private/home/dgopinath/data/seq2seq/samples/1.bvh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
