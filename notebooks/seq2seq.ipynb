{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these before executing\n",
    "# module load anaconda3/5.0.1\n",
    "# source activate fair_env_latest_py3_theano\n",
    "# module load cuda/9.0\n",
    "# module load NCCL/2.3.7-1-cuda.9.0\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import BVH\n",
    "import Animation\n",
    "from Quaternions import Quaternions\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANGLES = [45, 90, 135, 180]\n",
    "FREQUENCIES = [30, 60, 90, 120]\n",
    "TYPES = [\"sinusoidal\", \"sinusoidal_x\"] #'pendulum'\n",
    "NUM_JOINTS = 4\n",
    "NUM_COORDINATES = 3\n",
    "NUM_PREDICTIONS = 2 * NUM_JOINTS * NUM_COORDINATES\n",
    "BASE_PATH = \"/checkpoint/dgopinath/natural_gesture_generation/data/\"\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 128\n",
    "SUBSAMPLED_FRAMERATE = 30\n",
    "ACTION_TIME = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positions_wrt_base(anim):\n",
    "\n",
    "    # global transforms for each frame F and joint J\n",
    "    globals = Animation.transforms_global(anim)  # (F, J, 4, 4) ndarray\n",
    "    transforms_wrt_root = Animation.transforms_blank(anim)\n",
    "\n",
    "    for i in range(1, anim.shape[1]):  # modifies all joints except root/base\n",
    "        transforms_wrt_root[:, i] = Animation.transforms_multiply(Animation.transforms_inv(globals[:, 0]), globals[:, i])\n",
    "    transforms_wrt_root[:, 0] = globals[:, 0]  # root/base joint retains global position\n",
    "\n",
    "    positions_wrt_root = transforms_wrt_root[:, :, 0:3, 3]\n",
    "    rotations_wrt_root = Quaternions.from_transforms(transforms_wrt_root).angle_axis()\n",
    "\n",
    "    return rotations_wrt_root, positions_wrt_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGED FORMAT TO (ALL_ROTATIONS, ALL_POSITIONS)\n",
    "\n",
    "def extract_sequences(joint_rotations, joint_positions, frametime,\n",
    "                      stride=0.25, downsampled_hertz=30, window_length=5.0):\n",
    "\n",
    "    # initialize data structs, using CNN-like computation of size of input\n",
    "    J_p, J_r = joint_positions.shape[1], joint_rotations.shape[1],  # num of joints\n",
    "    animation_time = len(joint_rotations) * frametime\n",
    "    n = int(np.trunc((animation_time - window_length) / stride)) + 1  # sample size\n",
    "    num_joint_coordinates = joint_rotations.shape[2] + joint_positions.shape[2]\n",
    "    num_posture_dimensions = (J_p * joint_positions.shape[2]) + (J_r * joint_rotations.shape[2])  #J * num_joint_coordinates\n",
    "    T = int(np.trunc(downsampled_hertz * window_length))\n",
    "\n",
    "    datapoint_size = num_posture_dimensions * T\n",
    "    datapoints = np.zeros((n, datapoint_size), dtype=float)\n",
    "    datapoints_temporal = np.zeros((n, T, num_posture_dimensions), dtype=float)\n",
    "\n",
    "\n",
    "    current_frame_iterator = 0\n",
    "    input_hertz = int(np.trunc(1.0 / frametime))\n",
    "    step_size = int(np.trunc(input_hertz / downsampled_hertz))\n",
    "    sample_joint_positions = np.zeros((n, T, J_p, joint_positions.shape[2]))\n",
    "    sample_joint_rotations = np.zeros((n, T, J_r, joint_rotations.shape[2]))\n",
    "\n",
    "    for i in range(n):\n",
    "        # retrieve downsampled set of frames, for extracting character body posture over duration of sequence\n",
    "        posture_sequence_concatenated = []\n",
    "        posture_sequence_by_time = np.zeros((T, num_posture_dimensions))\n",
    "        for t in range(current_frame_iterator, (T * step_size + current_frame_iterator), step_size):\n",
    "\n",
    "            # get posture information for given (target) frame\n",
    "            posture = []\n",
    "            t_downsampled = int((t - current_frame_iterator)/float(step_size))\n",
    "\n",
    "            if J_p == J_r:\n",
    "                J = J_p\n",
    "                for j in range(J):  # all joints\n",
    "                    # rotation data\n",
    "                    sample_joint_rotations[i][t_downsampled][j] = joint_rotations[t][j]\n",
    "                    posture.extend(joint_rotations[t][j])\n",
    "                for j in range(J):  # all joints\n",
    "                    # position data\n",
    "                    sample_joint_positions[i][t_downsampled][j] = joint_positions[t][j]\n",
    "                    posture.extend(joint_positions[t][j])\n",
    "            else:\n",
    "                joints_pos, joints_rot = set(list(range(J_p))), set(list(range(J_r)))\n",
    "                J = joints_pos | joints_rot  # union of sets\n",
    "                for j in J:\n",
    "                    if j in joints_pos:  # joints considered for position data\n",
    "                        sample_joint_positions[i][t_downsampled][j] = joint_positions[t][j]\n",
    "                        posture.extend(joint_positions[t][j])\n",
    "                    if j in joints_rot:  # joints considered for rotation data\n",
    "                        sample_joint_rotations[i][t_downsampled][j] = joint_rotations[t][j]\n",
    "                        posture.extend(joint_rotations[t][j])\n",
    "\n",
    "            posture_sequence_concatenated.extend(posture)\n",
    "            posture_sequence_by_time[t_downsampled] = posture\n",
    "\n",
    "\n",
    "        # update datapoints struct with current iteration input\n",
    "        datapoints[i] = posture_sequence_concatenated\n",
    "        datapoints_temporal[i] = posture_sequence_by_time\n",
    "        current_frame_iterator += int(np.trunc(stride * input_hertz))\n",
    "\n",
    "    return datapoints, datapoints_temporal, sample_joint_rotations, sample_joint_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/dgopinath/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = defaultdict(list)\n",
    "for angle, freq, motion_type in product(ANGLES, FREQUENCIES, TYPES):\n",
    "    filename = os.path.join(BASE_PATH, motion_type, \"source\", f\"{angle}_{freq}.bvh\")\n",
    "    anim, joint_names, frametime = BVH.load(filename)\n",
    "    joint_rotations_root, joint_positions_root = positions_wrt_base(anim)\n",
    "    _, input_data, _, _ = extract_sequences(joint_rotations_root[1], joint_positions_root, frametime)\n",
    "    # input_data is of shape [num_sequences, seq_len, NUM_PREDICTIONS]\n",
    "    train, test = model_selection.train_test_split(input_data, train_size=0.8)\n",
    "    dataset[\"train\"].extend(train)\n",
    "    dataset[\"test\"].extend(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "        # self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, batch):\n",
    "        lstm_out, (lstm_hidden, lstm_cell) = self.lstm(batch) # batch.view(-1, , NUM_PREDICTIONS))\n",
    "        return lstm_hidden, lstm_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n",
    "        output = output.squeeze(0)\n",
    "        output = self.out(output)\n",
    "        return output, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = 'cuda'\n",
    "                \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        teacher_forcing_ratio = 0.5\n",
    "        max_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "        outputs = torch.zeros(max_len, batch_size, NUM_PREDICTIONS).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0,:]\n",
    "        for t in range(1, max_len):\n",
    "            input = input.unsqueeze(0)\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[t] if teacher_force else output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): LSTMEncoder(\n",
       "    (lstm): LSTM(24, 128, batch_first=True)\n",
       "  )\n",
       "  (decoder): LSTMDecoder(\n",
       "    (lstm): LSTM(24, 128)\n",
       "    (out): Linear(in_features=128, out_features=24, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = LSTMEncoder(NUM_PREDICTIONS, HIDDEN_DIM)\n",
    "dec = LSTMDecoder(NUM_PREDICTIONS, HIDDEN_DIM, NUM_PREDICTIONS)\n",
    "model = Seq2Seq(enc, dec).to('cuda')\n",
    "model.zero_grad()\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): LSTMEncoder(\n",
       "    (lstm): LSTM(24, 128, batch_first=True)\n",
       "  )\n",
       "  (decoder): LSTMDecoder(\n",
       "    (lstm): LSTM(24, 128)\n",
       "    (out): Linear(in_features=128, out_features=24, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    model.eval()\n",
    "    sequences = dataset[\"test\"]\n",
    "    with torch.no_grad():\n",
    "        batched_test_data_np = np.array(sequences)\n",
    "        batched_test_data_t = torch.from_numpy(batched_test_data_np).to(device='cuda')\n",
    "        batched_test_target_data_t = batched_test_data_t.transpose(0, 1)\n",
    "        outputs = model(batched_data_t, batched_target_data_t)\n",
    "        outputs = outputs.to(dtype=torch.double)\n",
    "        loss = criterion(outputs, batched_target_data_t)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, sequences):\n",
    "    model.eval()\n",
    "    batched_test_data_np = np.array(sequences)\n",
    "    batched_test_data_t = torch.from_numpy(batched_test_data_np).to(device='cuda')\n",
    "    batched_test_target_data_t = batched_test_data_t.transpose(0, 1)\n",
    "    outputs = model(batched_test_data_t, batched_test_target_data_t)\n",
    "    return outputs.transpose(0, 1).cpu().data.numpy(), batched_test_data_t.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/private/home/dgopinath/data/seq2seq/seq2seq.log\", \"w\") as f:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3)\n",
    "    for epoch in range(200):\n",
    "        f.write(f\"Epoch {epoch}\\n\")\n",
    "        f.write(f\"Dataset size {len(dataset['train'])}\\n\")\n",
    "        sequences = dataset[\"train\"]\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for i in range(int(len(sequences)/BATCH_SIZE)):\n",
    "            batched_data_np = np.array(sequences[i*BATCH_SIZE:(i+1)*BATCH_SIZE])\n",
    "            batched_data_t = torch.from_numpy(batched_data_np).to(device='cuda')\n",
    "            batched_target_data_t = batched_data_t.transpose(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batched_data_t, batched_target_data_t)\n",
    "            outputs = outputs.to(dtype=torch.double)\n",
    "            loss = criterion(outputs, batched_target_data_t)\n",
    "            loss.backward() # retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss = epoch_loss/(len(sequences)/BATCH_SIZE)\n",
    "        f.write(f\"Training loss {epoch_loss}\\n\")\n",
    "        val_loss = eval(model)\n",
    "        f.write(f\"Validation loss {val_loss}\\n\")\n",
    "        scheduler.step(val_loss)\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(model.state_dict(), f\"/private/home/dgopinath/data/seq2seq/models/{epoch}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "training_loss, validation_loss = [], []\n",
    "with open(\"/private/home/dgopinath/data/seq2seq/seq2seq.log\") as f:\n",
    "    for line in f:\n",
    "        if \"Epoch\" in line or \"Dataset\" in line:\n",
    "            continue\n",
    "        if \"Training loss\" in line:\n",
    "            training_loss.append(float(line.split(\" \")[2]))\n",
    "        if \"Validation loss\" in line:\n",
    "            validation_loss.append(float(line.split(\" \")[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(training_loss)), training_loss)\n",
    "plt.plot(range(len(training_loss)), validation_loss)\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.savefig(\"/private/home/dgopinath/data/seq2seq_1layer_batch64.svg\", format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/private/home/dgopinath/data/seq2seq_losses.csv\", \"w\") as f:\n",
    "    f.write(f\"Epoch,Training_Loss,Validation_Loss\\n\")\n",
    "    for num, (t_loss, v_loss) in enumerate(zip(training_loss, validation_loss)):\n",
    "        f.write(f\"{num},{t_loss},{v_loss}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_data(tensor_struct):\n",
    "    if isinstance(tensor_struct, np.ndarray):\n",
    "        return tensor_struct\n",
    "\n",
    "    elif isinstance(tensor_struct, list) or isinstance(tensor_struct, tuple):\n",
    "        return np.array(tensor_struct)\n",
    "\n",
    "    elif torch.is_tensor(tensor_struct):\n",
    "        if tensor_struct.device.type == 'cuda':\n",
    "            data_np = tensor_struct.data.cpu().numpy()\n",
    "        else:\n",
    "            data_np = tensor_struct.data.numpy()\n",
    "        return data_np\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positions_global_from_base(rotations_joint_wrt_base, positions_joint_wrt_base):\n",
    "    \"\"\"\n",
    "    At each time t, for each j in J:  P_j^(global) = R_base * P_j^(base)  + P_base\n",
    "    :param anim:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    positions_base = positions_joint_wrt_base[:, 0]\n",
    "    rotations_base_transforms = rotations_joint_wrt_base.transforms()[:, 0]\n",
    "    T, J = positions_joint_wrt_base.shape[0], positions_joint_wrt_base.shape[1]\n",
    "    positions_global = np.zeros((T, J, positions_joint_wrt_base.shape[2]))\n",
    "\n",
    "    for t in range(T):\n",
    "        positions_global[t, 0] = positions_base[t]  # simply copy base joint position\n",
    "        for j in range(1, J):\n",
    "            positions_global[t,j] = np.matmul(rotations_base_transforms[t], positions_joint_wrt_base[t,j]) + \\\n",
    "                positions_base[t]\n",
    "            pass\n",
    "    return positions_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_motion_sequence(T, current_hertz, desired_hertz, joint_rotations, joint_positions):\n",
    "    upsample_rate = int(desired_hertz / current_hertz)\n",
    "    T_upsampled = T * upsample_rate\n",
    "    J_p, J_r = joint_positions.shape[1], joint_rotations.shape[1]\n",
    "    num_position_coordinates, num_rotation_coordinates = joint_positions.shape[2], joint_rotations.shape[2]\n",
    "\n",
    "\n",
    "    joint_rotations_upsampled = np.zeros(shape=(T_upsampled, J_r, num_rotation_coordinates))\n",
    "    joint_positions_upsampled = np.zeros(shape=(T_upsampled, J_p, num_position_coordinates))\n",
    "\n",
    "    for t in range(T):\n",
    "\n",
    "        t_upsampled = t * upsample_rate\n",
    "        joint_rotations_upsampled[t_upsampled] = joint_rotations[t]\n",
    "        joint_positions_upsampled[t_upsampled] = joint_positions[t]\n",
    "\n",
    "        if (t + 1) < T:\n",
    "            interpolated_rot_increment = (joint_rotations[t + 1] - joint_rotations[t]) / upsample_rate\n",
    "            interpolated_pos_increment = (joint_positions[t + 1] - joint_positions[t]) / upsample_rate\n",
    "            for i in range(1, upsample_rate):\n",
    "                joint_rotations_upsampled[t_upsampled + i] = joint_rotations[t] + i * interpolated_rot_increment\n",
    "                joint_positions_upsampled[t_upsampled + i] = joint_positions[t] + i * interpolated_pos_increment\n",
    "\n",
    "            pass\n",
    "\n",
    "    return joint_rotations_upsampled, joint_positions_upsampled\n",
    "\n",
    "    \n",
    "def decompose_axis_angles(axis_angles):\n",
    "    \"\"\"\n",
    "    Input dimensions: T x J x num-coordinates\n",
    "    Output dimensions:\n",
    "        angles: T x J\n",
    "        axes: T x J x num-coords\n",
    "    \"\"\"\n",
    "\n",
    "    angles = np.linalg.norm(get_tensor_data(axis_angles), axis=2)\n",
    "    axes = np.zeros_like(axis_angles)\n",
    "\n",
    "    for t in range(axis_angles.shape[0]):  # traverse through time/datapoints\n",
    "        for j in range(axis_angles.shape[1]):  # traverse through joints\n",
    "            if angles[t, j] < 1e-5:\n",
    "                axes[t, j, :] = [1, 0, 0]\n",
    "            else:\n",
    "                axes[t, j, :] = axis_angles[t, j, :] / angles[t, j]\n",
    "    return angles, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character():\n",
    "    \"\"\"Defines character profile and metadata\"\"\"\n",
    "\n",
    "    def __init__(self, id, orients=None, offsets=None, parents=None):\n",
    "\n",
    "        self.id = id\n",
    "\n",
    "        self.orients = orients\n",
    "        self.offsets = offsets\n",
    "        self.parents = parents\n",
    "\n",
    "\n",
    "        self.J_p = 0\n",
    "        self.J_r = 0\n",
    "        self.num_rotation_coordinates = 3\n",
    "        self.num_position_coordinates = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### Setters ####\n",
    "    def update_metadata(self, anim):\n",
    "        self.set_orients(anim.orients)\n",
    "        self.set_offsets(anim.offsets)\n",
    "        self.set_parents(anim.parents)\n",
    "\n",
    "\n",
    "    def set_orients(self, orients):\n",
    "        self.orients = orients\n",
    "\n",
    "\n",
    "    def set_offsets(self, offsets):\n",
    "        self.offsets = offsets\n",
    "\n",
    "\n",
    "    def set_parents(self, parents):\n",
    "        self.parents = parents\n",
    "\n",
    "\n",
    "\n",
    "    def set_joint_information(self, positions, rotations):\n",
    "\n",
    "        self.J_p = positions.shape[2]\n",
    "        self.num_position_coordinates = positions.shape[3]\n",
    "\n",
    "        self.J_r = rotations.shape[2]\n",
    "        self.num_rotation_coordinates = rotations.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LSTMEncoder(NUM_PREDICTIONS, HIDDEN_DIM)\n",
    "dec = LSTMDecoder(NUM_PREDICTIONS, HIDDEN_DIM, NUM_PREDICTIONS)\n",
    "loaded_model = Seq2Seq(enc, dec).to('cuda')\n",
    "loaded_model.load_state_dict(torch.load(\"/private/home/dgopinath/data/seq2seq/models/100.model\"))\n",
    "loaded_model.eval()\n",
    "loaded_model.double()\n",
    "\n",
    "outputs, inputs = generate(loaded_model, dataset[\"test\"])\n",
    "# source_character = Character('source')\n",
    "# T = len(joint_rotations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(outputs):\n",
    "    T = outputs.shape[1]\n",
    "    upsampled_sequences = []\n",
    "    for seq_idx in range(outputs.shape[0]):\n",
    "        rotations = outputs[seq_idx, :, 0:int(NUM_PREDICTIONS/2)].reshape(T, -1, 3)\n",
    "        positions = outputs[seq_idx, :, int(NUM_PREDICTIONS/2): NUM_PREDICTIONS].reshape(T, -1, 3)\n",
    "        rotations_wrt_base, positions_wrt_base = upsample_motion_sequence(T, 30, 120, rotations, positions)\n",
    "        joint_rotations_angles, joint_rotations_axis = decompose_axis_angles(rotations_wrt_base)\n",
    "        joint_rotations_quats = Quaternions.from_angle_axis(joint_rotations_angles, joint_rotations_axis)\n",
    "        upsampled_sequences.append((joint_rotations_quats, positions_wrt_base))\n",
    "    return upsampled_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalize(outputs, upsampled_sequences):\n",
    "    global_sequences = []\n",
    "    for seq_idx in range(outputs.shape[0]):\n",
    "        global_sequences.append(positions_global_from_base(*upsampled_sequences[seq_idx]))\n",
    "    return global_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bvh(globalized, upsampled, idx, filepath, input_file=BASE_PATH + \"/sinusoidal/source/90_90.bvh\"):\n",
    "    source_character = Character('source')\n",
    "    anim, _, _ = BVH.load(input_file)\n",
    "    source_character.update_metadata(anim)\n",
    "    input_anim = Animation.Animation(upsampled[idx][0], globalized[idx], source_character.orients, source_character.offsets, source_character.parents)\n",
    "    BVH.save(filepath, input_anim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled = upsample(inputs)\n",
    "globalized = globalize(inputs, upsampled)\n",
    "save_bvh(globalized, upsampled, 1, \"/private/home/dgopinath/data/seq2seq/samples/1.bvh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 4, 3)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "        0.        ,  1.        , -3.        ,  6.        ,  5.        ,\n",
       "        0.        ,  5.        ,  0.        , -5.        ,  5.        ,\n",
       "        0.        , -5.67485316,  7.92311019,  0.        ])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0][0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.        ,  6.        ,  5.        ],\n",
       "       [-3.        , 11.        ,  5.        ],\n",
       "       [-8.        , 11.        ,  5.        ],\n",
       "       [-8.67485316, 13.92311019,  5.        ]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_inputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim, _, frametime = BVH.load(BASE_PATH + \"/sinusoidal/source/90_90.bvh\")\n",
    "joint_rotations_root, joint_positions_root = positions_wrt_base(anim)\n",
    "_, input_data, _, _ = extract_sequences(joint_rotations_root[1], joint_positions_root, frametime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, i = generate(loaded_model, np.array([input_data[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled = upsample(o)\n",
    "globalized = globalize(o, upsampled)\n",
    "save_bvh(globalized, upsampled, 0, \"/private/home/dgopinath/data/seq2seq/samples/output_90_90.bvh\", input_file=BASE_PATH + \"/sinusoidal/source/90_90.bvh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_anim, _, _ = BVH.load(\"/private/home/dgopinath/data/seq2seq/samples/output_90_90.bvh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.,  6.,  5.],\n",
       "       [ 0.,  5.,  0.],\n",
       "       [-5.,  0.,  0.],\n",
       "       [ 0.,  3.,  0.]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anim.positions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.508132,  2.988158,  2.493383],\n",
       "       [ 0.      ,  5.      ,  0.      ],\n",
       "       [-5.      ,  0.      ,  0.      ],\n",
       "       [ 0.      ,  3.      ,  0.      ]])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_anim.positions[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.8746748e-02, -8.5774055e-03, -6.3152681e-03, -3.9731123e-02,\n",
       "        1.0798303e-02,  1.4542969e-03,  7.6455675e-02,  1.9589721e-03,\n",
       "        5.4101890e-01,  6.0611557e-02,  8.2202367e-03,  5.5778843e-01,\n",
       "       -3.0162637e+00,  5.9763150e+00,  4.9867654e+00,  4.5793336e-02,\n",
       "        4.9385819e+00,  8.8929925e-03, -4.9574056e+00,  4.9799585e+00,\n",
       "       -1.2611205e-02, -5.3333430e+00,  7.5692821e+00,  1.2127520e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
